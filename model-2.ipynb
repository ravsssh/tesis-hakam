{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8603f5a",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488fface",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Darts time series\n",
    "from darts import TimeSeries\n",
    "from darts.models import RandomForest\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.metrics import mape, rmse, mae, r2_score\n",
    "\n",
    "# SHAP for feature importance\n",
    "import shap\n",
    "\n",
    "# Sklearn utilities\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Plot style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda3a192",
   "metadata": {},
   "source": [
    "## 2. Load and Merge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef30dfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all datasets\n",
    "print(\"=\"*60)\n",
    "print(\"LOADING DATASETS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# IHSG (target) - Indonesian format dd/mm/yyyy\n",
    "ihsg = pd.read_csv('ihsg_daily.csv')\n",
    "ihsg['Date'] = pd.to_datetime(ihsg['Date'], format='%d/%m/%Y')\n",
    "ihsg.columns = ['Date', 'IHSG']\n",
    "print(f\"IHSG: {len(ihsg)} rows, {ihsg['Date'].min()} to {ihsg['Date'].max()}\")\n",
    "\n",
    "# STI - Indonesian format dd/mm/yyyy\n",
    "sti = pd.read_csv('STI.csv')\n",
    "sti['Date'] = pd.to_datetime(sti['Date'], format='%d/%m/%Y')\n",
    "sti.columns = ['Date', 'STI']\n",
    "print(f\"STI: {len(sti)} rows, {sti['Date'].min()} to {sti['Date'].max()}\")\n",
    "\n",
    "# Commodities - US format mm/dd/yyyy\n",
    "coal = pd.read_csv('Coal.csv')\n",
    "coal['Date'] = pd.to_datetime(coal['Date'], format='%m/%d/%Y')\n",
    "coal.columns = ['Date', 'Coal']\n",
    "print(f\"Coal: {len(coal)} rows, {coal['Date'].min()} to {coal['Date'].max()}\")\n",
    "\n",
    "copper = pd.read_csv('Copper.csv')\n",
    "copper['Date'] = pd.to_datetime(copper['Date'], format='%m/%d/%Y')\n",
    "copper.columns = ['Date', 'Copper']\n",
    "print(f\"Copper: {len(copper)} rows, {copper['Date'].min()} to {copper['Date'].max()}\")\n",
    "\n",
    "silver = pd.read_csv('Silver.csv')\n",
    "silver['Date'] = pd.to_datetime(silver['Date'], format='%m/%d/%Y')\n",
    "silver.columns = ['Date', 'Silver']\n",
    "print(f\"Silver: {len(silver)} rows, {silver['Date'].min()} to {silver['Date'].max()}\")\n",
    "\n",
    "tin = pd.read_csv('Tin.csv')\n",
    "tin['Date'] = pd.to_datetime(tin['Date'], format='%m/%d/%Y')\n",
    "tin.columns = ['Date', 'Tin']\n",
    "print(f\"Tin: {len(tin)} rows, {tin['Date'].min()} to {tin['Date'].max()}\")\n",
    "\n",
    "nickel = pd.read_csv('Nickel.csv')\n",
    "nickel['Date'] = pd.to_datetime(nickel['Date'], format='%m/%d/%Y')\n",
    "nickel.columns = ['Date', 'Nickel']\n",
    "print(f\"Nickel: {len(nickel)} rows, {nickel['Date'].min()} to {nickel['Date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f68da0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all datasets on Date (inner join to get common dates)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MERGING DATASETS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Start with IHSG as base\n",
    "df = ihsg.copy()\n",
    "\n",
    "# Merge each dataset\n",
    "for dataset, name in [(sti, 'STI'), (coal, 'Coal'), (copper, 'Copper'), \n",
    "                       (silver, 'Silver'), (tin, 'Tin'), (nickel, 'Nickel')]:\n",
    "    df = df.merge(dataset, on='Date', how='inner')\n",
    "    print(f\"After merging {name}: {len(df)} rows\")\n",
    "\n",
    "# Sort by date\n",
    "df = df.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "print(f\"\\n‚úÖ Final merged dataset: {len(df)} rows\")\n",
    "print(f\"Date range: {df['Date'].min()} to {df['Date'].max()}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8276b747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display merged data\n",
    "print(\"=\"*60)\n",
    "print(\"MERGED DATA PREVIEW\")\n",
    "print(\"=\"*60)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a65ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types and missing values\n",
    "print(\"=\"*60)\n",
    "print(\"DATA TYPES & MISSING VALUES\")\n",
    "print(\"=\"*60)\n",
    "df.info()\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MISSING VALUES PER COLUMN\")\n",
    "print(\"=\"*60)\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c7a6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics\n",
    "print(\"=\"*60)\n",
    "print(\"DESCRIPTIVE STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a950e9e5",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9b2153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Time series plots\n",
    "fig, axes = plt.subplots(4, 2, figsize=(16, 14))\n",
    "fig.suptitle('Time Series Visualization - IHSG, STI, and Commodity Prices', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Define colors\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2']\n",
    "columns = ['IHSG', 'STI', 'Coal', 'Copper', 'Silver', 'Tin', 'Nickel']\n",
    "\n",
    "for idx, (col, color) in enumerate(zip(columns, colors)):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    ax.plot(df['Date'], df[col], color=color, linewidth=0.8, alpha=0.9)\n",
    "    ax.set_title(f'{col}', fontsize=12)\n",
    "    ax.set_ylabel('Price/Index')\n",
    "    ax.fill_between(df['Date'], df[col], alpha=0.2, color=color)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Hide the last empty subplot\n",
    "axes[3, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model2_eda_timeseries.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a16099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Analysis\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "corr_matrix = df.drop('Date', axis=1).corr()\n",
    "\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.3f', cmap='RdBu_r', \n",
    "            center=0, square=True, linewidths=0.5, ax=ax,\n",
    "            annot_kws={'size': 10})\n",
    "\n",
    "ax.set_title('Correlation Matrix - IHSG, STI, and Commodities', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('model2_correlation_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CORRELATION WITH IHSG\")\n",
    "print(\"=\"*60)\n",
    "ihsg_corr = corr_matrix['IHSG'].drop('IHSG').sort_values(ascending=False)\n",
    "for var, corr in ihsg_corr.items():\n",
    "    direction = \"‚Üë Positive\" if corr > 0 else \"‚Üì Negative\"\n",
    "    strength = \"Strong\" if abs(corr) > 0.7 else \"Moderate\" if abs(corr) > 0.4 else \"Weak\"\n",
    "    print(f\"{var:15} : {corr:+.4f} ({strength} {direction})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd167c7",
   "metadata": {},
   "source": [
    "## 4. Create Darts TimeSeries Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1384402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target and covariate columns\n",
    "TARGET_COL = 'IHSG'\n",
    "COVARIATE_COLS = ['STI', 'Coal', 'Copper', 'Silver', 'Tin', 'Nickel']\n",
    "\n",
    "# Set Date as index with business day frequency\n",
    "df_ts = df.set_index('Date')\n",
    "\n",
    "# Create target TimeSeries\n",
    "target_series = TimeSeries.from_dataframe(\n",
    "    df_ts, \n",
    "    value_cols=TARGET_COL,\n",
    "    fill_missing_dates=False  # Daily data may have gaps (weekends/holidays)\n",
    ")\n",
    "\n",
    "# Create covariates TimeSeries\n",
    "covariates = TimeSeries.from_dataframe(\n",
    "    df_ts,\n",
    "    value_cols=COVARIATE_COLS,\n",
    "    fill_missing_dates=False\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DARTS TIMESERIES CREATED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Target Series (IHSG):\")\n",
    "print(f\"  - Start: {target_series.start_time()}\")\n",
    "print(f\"  - End: {target_series.end_time()}\")\n",
    "print(f\"  - Length: {len(target_series)} time steps\")\n",
    "print(f\"\\nCovariates:\")\n",
    "print(f\"  - Components: {covariates.components.tolist()}\")\n",
    "print(f\"  - Length: {len(covariates)} time steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b519dd7",
   "metadata": {},
   "source": [
    "## 5. Train/Test Split and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d7655e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test Split (80/20)\n",
    "TRAIN_RATIO = 0.8\n",
    "split_point = int(len(target_series) * TRAIN_RATIO)\n",
    "\n",
    "train_target = target_series[:split_point]\n",
    "test_target = target_series[split_point:]\n",
    "\n",
    "train_cov = covariates[:split_point]\n",
    "test_cov = covariates[split_point:]\n",
    "\n",
    "# Scale the data using Darts Scaler\n",
    "scaler_target = Scaler()\n",
    "scaler_cov = Scaler()\n",
    "\n",
    "# Fit on training data only to avoid data leakage\n",
    "train_target_scaled = scaler_target.fit_transform(train_target)\n",
    "test_target_scaled = scaler_target.transform(test_target)\n",
    "\n",
    "train_cov_scaled = scaler_cov.fit_transform(train_cov)\n",
    "test_cov_scaled = scaler_cov.transform(test_cov)\n",
    "\n",
    "# Full scaled series for prediction\n",
    "target_scaled = scaler_target.transform(target_series)\n",
    "cov_scaled = scaler_cov.transform(covariates)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAIN/TEST SPLIT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Train Period: {train_target.start_time()} to {train_target.end_time()} ({len(train_target)} days)\")\n",
    "print(f\"Test Period:  {test_target.start_time()} to {test_target.end_time()} ({len(test_target)} days)\")\n",
    "print(f\"\\nTrain/Test Ratio: {TRAIN_RATIO*100:.0f}% / {(1-TRAIN_RATIO)*100:.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08307170",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Tuning with GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c081bf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grid for RandomForest\n",
    "# For daily data, use shorter lags (days instead of months)\n",
    "param_grid = {\n",
    "    'lags': [5, 10, 21],                      # Target lags (days) - ~1 week, 2 weeks, 1 month\n",
    "    'lags_past_covariates': [5, 10, 21],     # Covariate lags\n",
    "    'n_estimators': [100, 200, 300],         # Number of trees\n",
    "    'max_depth': [5, 10, None],              # Maximum tree depth\n",
    "}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"HYPERPARAMETER GRID\")\n",
    "print(\"=\"*60)\n",
    "for param, values in param_grid.items():\n",
    "    print(f\"{param}: {values}\")\n",
    "\n",
    "total_combinations = 1\n",
    "for values in param_grid.values():\n",
    "    total_combinations *= len(values)\n",
    "print(f\"\\nTotal combinations to evaluate: {total_combinations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2238cbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform GridSearch\n",
    "print(\"=\"*60)\n",
    "print(\"RUNNING GRIDSEARCH (this may take several minutes for daily data...)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_model, best_params, best_score = RandomForest.gridsearch(\n",
    "    parameters=param_grid,\n",
    "    series=train_target_scaled,\n",
    "    past_covariates=cov_scaled,  # Full covariates to cover validation period\n",
    "    val_series=test_target_scaled,\n",
    "    metric=mape,\n",
    "    verbose=True,\n",
    "    n_jobs=-1,  # Use all CPU cores\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GRIDSEARCH RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Best MAPE Score: {best_score:.4f}%\")\n",
    "print(f\"\\nBest Hyperparameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"  {param}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db229c0",
   "metadata": {},
   "source": [
    "## 7. Train Best Model and Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45e04cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the best model with optimal hyperparameters\n",
    "final_model = RandomForest(\n",
    "    lags=best_params['lags'],\n",
    "    lags_past_covariates=best_params['lags_past_covariates'],\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    max_depth=best_params['max_depth'],\n",
    "    output_chunk_length=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit on training data\n",
    "final_model.fit(train_target_scaled, past_covariates=train_cov_scaled)\n",
    "\n",
    "# Generate predictions on test set\n",
    "n_test = len(test_target_scaled)\n",
    "predictions_scaled = final_model.predict(n=n_test, past_covariates=cov_scaled)\n",
    "\n",
    "# Inverse transform to original scale\n",
    "predictions = scaler_target.inverse_transform(predictions_scaled)\n",
    "test_actual = scaler_target.inverse_transform(test_target_scaled)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL TRAINED AND PREDICTIONS GENERATED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Prediction Period: {predictions.start_time()} to {predictions.end_time()}\")\n",
    "print(f\"Number of predictions: {len(predictions)} trading days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6953c79e",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a896c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics\n",
    "mape_score = mape(test_actual, predictions)\n",
    "rmse_score = rmse(test_actual, predictions)\n",
    "mae_score = mae(test_actual, predictions)\n",
    "r2 = r2_score(test_actual, predictions)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL EVALUATION METRICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"MAPE (Mean Absolute Percentage Error): {mape_score:.4f}%\")\n",
    "print(f\"RMSE (Root Mean Square Error):         {rmse_score:.4f}\")\n",
    "print(f\"MAE (Mean Absolute Error):             {mae_score:.4f}\")\n",
    "print(f\"R¬≤ Score:                              {r2:.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Interpretation\n",
    "print(\"\\nüìä INTERPRETATION:\")\n",
    "if mape_score < 5:\n",
    "    print(f\"  ‚úÖ MAPE < 5%: Excellent forecasting accuracy\")\n",
    "elif mape_score < 10:\n",
    "    print(f\"  ‚úÖ MAPE < 10%: Good forecasting accuracy\")\n",
    "elif mape_score < 20:\n",
    "    print(f\"  ‚ö†Ô∏è MAPE < 20%: Reasonable forecasting accuracy\")\n",
    "else:\n",
    "    print(f\"  ‚ùå MAPE > 20%: Poor forecasting accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3d163a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Actual vs Predicted\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 10))\n",
    "\n",
    "# Plot 1: Full time series with predictions\n",
    "ax1 = axes[0]\n",
    "ax1.plot(train_target.time_index, train_target.values(), \n",
    "         label='Training Data', color='#1f77b4', linewidth=0.8, alpha=0.8)\n",
    "ax1.plot(test_actual.time_index, test_actual.values(), \n",
    "         label='Actual (Test)', color='#2ca02c', linewidth=1.5)\n",
    "ax1.plot(predictions.time_index, predictions.values(), \n",
    "         label='Predicted', color='#d62728', linewidth=1.5, linestyle='--')\n",
    "ax1.axvline(x=train_target.end_time(), color='gray', linestyle=':', alpha=0.7, label='Train/Test Split')\n",
    "ax1.set_title('IHSG Prediction - Random Forest Model (Daily Data)', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('IHSG Index')\n",
    "ax1.legend(loc='upper left')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Test period close-up\n",
    "ax2 = axes[1]\n",
    "ax2.plot(test_actual.time_index, test_actual.values(), \n",
    "         label='Actual', color='#2ca02c', linewidth=1.5)\n",
    "ax2.plot(predictions.time_index, predictions.values(), \n",
    "         label='Predicted', color='#d62728', linewidth=1.5, linestyle='--')\n",
    "ax2.fill_between(test_actual.time_index, \n",
    "                  test_actual.values().flatten(), \n",
    "                  predictions.values().flatten(), \n",
    "                  alpha=0.3, color='gray', label='Error')\n",
    "ax2.set_title(f'Test Period: Actual vs Predicted (MAPE: {mape_score:.2f}%)', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Date')\n",
    "ax2.set_ylabel('IHSG Index')\n",
    "ax2.legend(loc='upper left')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model2_prediction_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923add37",
   "metadata": {},
   "source": [
    "## 9. Feature Importance with SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc8d1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare feature matrix for SHAP\n",
    "lags = best_params['lags']\n",
    "lags_cov = best_params['lags_past_covariates']\n",
    "\n",
    "# Create lagged feature dataframe\n",
    "def create_lagged_features(target_df, cov_df, target_lags, cov_lags):\n",
    "    \"\"\"Create lagged features for SHAP analysis\"\"\"\n",
    "    max_lag = max(target_lags, cov_lags)\n",
    "    features = pd.DataFrame(index=target_df.index[max_lag:])\n",
    "    \n",
    "    # Target lags\n",
    "    for lag in range(1, target_lags + 1):\n",
    "        features[f'IHSG_lag{lag}'] = target_df['IHSG'].shift(lag).values[max_lag:]\n",
    "    \n",
    "    # Covariate lags\n",
    "    for col in cov_df.columns:\n",
    "        for lag in range(1, cov_lags + 1):\n",
    "            features[f'{col}_lag{lag}'] = cov_df[col].shift(lag).values[max_lag:]\n",
    "    \n",
    "    return features.dropna()\n",
    "\n",
    "# Prepare data\n",
    "target_df = df.set_index('Date')[['IHSG']]\n",
    "cov_df = df.set_index('Date')[COVARIATE_COLS]\n",
    "\n",
    "X_features = create_lagged_features(target_df, cov_df, lags, lags_cov)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FEATURE MATRIX FOR SHAP\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Feature matrix shape: {X_features.shape}\")\n",
    "print(f\"\\nFeatures ({len(X_features.columns)}):\")\n",
    "for i, col in enumerate(X_features.columns, 1):\n",
    "    print(f\"  {i}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aa8a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a standalone RandomForest model for SHAP analysis\n",
    "\n",
    "# Prepare target (shifted to align with features)\n",
    "max_lag = max(lags, lags_cov)\n",
    "y_target = target_df['IHSG'].values[max_lag + 1:]  # +1 for next day prediction\n",
    "X_train_shap = X_features.iloc[:-1].values  # Remove last row to match y_target length\n",
    "\n",
    "# Ensure alignment\n",
    "min_len = min(len(X_train_shap), len(y_target))\n",
    "X_train_shap = X_train_shap[:min_len]\n",
    "y_target = y_target[:min_len]\n",
    "\n",
    "# Train RF model for SHAP\n",
    "rf_shap = RandomForestRegressor(\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    max_depth=best_params['max_depth'],\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_shap.fit(X_train_shap, y_target)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"RANDOM FOREST FOR SHAP ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model fitted on {X_train_shap.shape[0]} samples with {X_train_shap.shape[1]} features\")\n",
    "print(f\"R¬≤ Score: {rf_shap.score(X_train_shap, y_target):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee00550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Analysis\n",
    "print(\"=\"*60)\n",
    "print(\"COMPUTING SHAP VALUES (this may take a moment...)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create SHAP explainer\n",
    "explainer = shap.TreeExplainer(rf_shap)\n",
    "\n",
    "# Use a sample for SHAP (full dataset can be slow for daily data)\n",
    "sample_size = min(1000, len(X_train_shap))\n",
    "np.random.seed(42)\n",
    "sample_idx = np.random.choice(len(X_train_shap), sample_size, replace=False)\n",
    "X_sample = X_train_shap[sample_idx]\n",
    "\n",
    "# Calculate SHAP values\n",
    "shap_values = explainer.shap_values(X_sample)\n",
    "\n",
    "# Create DataFrame with feature names\n",
    "feature_names = X_features.columns[:X_train_shap.shape[1]].tolist()\n",
    "X_sample_df = pd.DataFrame(X_sample, columns=feature_names)\n",
    "\n",
    "print(f\"‚úÖ SHAP values computed on {sample_size} samples!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc995418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Summary Plot (Bar)\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "shap.summary_plot(shap_values, X_sample_df, plot_type=\"bar\", show=False, max_display=20)\n",
    "plt.title('SHAP Feature Importance - Mean |SHAP Value| (Top 20)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('model2_shap_importance_bar.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903ef8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Summary Plot (Beeswarm)\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "shap.summary_plot(shap_values, X_sample_df, show=False, max_display=20)\n",
    "plt.title('SHAP Feature Importance - Beeswarm Plot (Top 20)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('model2_shap_importance_beeswarm.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9663aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate aggregated feature importance by variable (not by lag)\n",
    "feature_importance_by_var = {}\n",
    "\n",
    "for col in X_sample_df.columns:\n",
    "    # Extract variable name (remove lag suffix)\n",
    "    if '_lag' in col:\n",
    "        var_name = col.rsplit('_lag', 1)[0]\n",
    "    else:\n",
    "        var_name = col\n",
    "    \n",
    "    col_idx = list(X_sample_df.columns).index(col)\n",
    "    importance = np.abs(shap_values[:, col_idx]).mean()\n",
    "    \n",
    "    if var_name not in feature_importance_by_var:\n",
    "        feature_importance_by_var[var_name] = 0\n",
    "    feature_importance_by_var[var_name] += importance\n",
    "\n",
    "# Sort by importance\n",
    "sorted_importance = dict(sorted(feature_importance_by_var.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"AGGREGATED FEATURE IMPORTANCE BY VARIABLE\")\n",
    "print(\"=\"*60)\n",
    "total_importance = sum(sorted_importance.values())\n",
    "for var, importance in sorted_importance.items():\n",
    "    pct = (importance / total_importance) * 100\n",
    "    bar = \"‚ñà\" * int(pct / 2)\n",
    "    print(f\"{var:15} : {importance:8.4f} ({pct:5.2f}%) {bar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100aecb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Aggregated Feature Importance\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "vars_names = list(sorted_importance.keys())\n",
    "importances = list(sorted_importance.values())\n",
    "\n",
    "# Color coding: IHSG = blue, STI = green, Commodities = orange\n",
    "colors = []\n",
    "for v in vars_names:\n",
    "    if 'IHSG' in v:\n",
    "        colors.append('#1f77b4')\n",
    "    elif 'STI' in v:\n",
    "        colors.append('#2ca02c')\n",
    "    else:\n",
    "        colors.append('#ff7f0e')\n",
    "\n",
    "bars = ax.barh(vars_names[::-1], importances[::-1], color=colors[::-1])\n",
    "ax.set_xlabel('Mean |SHAP Value|', fontsize=12)\n",
    "ax.set_title('Feature Importance by Variable (Aggregated across lags)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, importances[::-1]):\n",
    "    ax.text(val + max(importances)*0.01, bar.get_y() + bar.get_height()/2, \n",
    "            f'{val:.2f}', va='center', fontsize=10)\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='#1f77b4', label='Target (IHSG)'),\n",
    "    Patch(facecolor='#2ca02c', label='Regional Index (STI)'),\n",
    "    Patch(facecolor='#ff7f0e', label='Commodities')\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model2_feature_importance_aggregated.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d7783f",
   "metadata": {},
   "source": [
    "## 10. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04527f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary\n",
    "print(\"=\"*70)\n",
    "print(\"HASIL PENELITIAN MODEL 2: IHSG DENGAN KOMODITAS DAN STI\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüìä DATA:\")\n",
    "print(f\"   ‚Ä¢ Periode Data     : {df['Date'].min().strftime('%d %B %Y')} - {df['Date'].max().strftime('%d %B %Y')}\")\n",
    "print(f\"   ‚Ä¢ Total Observasi  : {len(df)} hari trading\")\n",
    "print(f\"   ‚Ä¢ Train/Test Split : {TRAIN_RATIO*100:.0f}% / {(1-TRAIN_RATIO)*100:.0f}%\")\n",
    "\n",
    "print(\"\\nüéØ TARGET:\")\n",
    "print(f\"   ‚Ä¢ Variabel Target  : IHSG (Indeks Harga Saham Gabungan) - Daily\")\n",
    "\n",
    "print(\"\\nüìà COVARIATES:\")\n",
    "print(\"   Regional Index:\")\n",
    "print(\"     1. STI (Straits Times Index)\")\n",
    "print(\"   Commodities:\")\n",
    "for i, col in enumerate(['Coal', 'Copper', 'Silver', 'Tin', 'Nickel'], 2):\n",
    "    print(f\"     {i}. {col}\")\n",
    "\n",
    "print(\"\\n‚öôÔ∏è BEST HYPERPARAMETERS (via GridSearch):\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"   ‚Ä¢ {param}: {value}\")\n",
    "\n",
    "print(\"\\nüìè MODEL PERFORMANCE:\")\n",
    "print(f\"   ‚Ä¢ MAPE  : {mape_score:.4f}%\")\n",
    "print(f\"   ‚Ä¢ RMSE  : {rmse_score:.4f}\")\n",
    "print(f\"   ‚Ä¢ MAE   : {mae_score:.4f}\")\n",
    "print(f\"   ‚Ä¢ R¬≤    : {r2:.4f}\")\n",
    "\n",
    "print(\"\\nüîç FEATURE IMPORTANCE (SHAP - Top 3):\")\n",
    "for i, (var, imp) in enumerate(list(sorted_importance.items())[:3], 1):\n",
    "    pct = (imp / total_importance) * 100\n",
    "    print(f\"   {i}. {var}: {pct:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Source: Author's calculation, 2025\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bf6224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV\n",
    "results_df = pd.DataFrame({\n",
    "    'Date': test_actual.time_index,\n",
    "    'Actual_IHSG': test_actual.values().flatten(),\n",
    "    'Predicted_IHSG': predictions.values().flatten(),\n",
    "    'Error': (predictions.values().flatten() - test_actual.values().flatten()),\n",
    "    'APE_%': np.abs((predictions.values().flatten() - test_actual.values().flatten()) / test_actual.values().flatten()) * 100\n",
    "})\n",
    "\n",
    "results_df.to_csv('model2_predictions.csv', index=False)\n",
    "print(\"‚úÖ Predictions saved to 'model2_predictions.csv'\")\n",
    "\n",
    "# Also save the merged dataset\n",
    "df.to_csv('model2_merged_data.csv', index=False)\n",
    "print(\"‚úÖ Merged dataset saved to 'model2_merged_data.csv'\")\n",
    "\n",
    "# Display prediction results\n",
    "print(\"\\nPrediction Results (first 10 rows):\")\n",
    "results_df.head(10).round(2)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
